{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80d3e0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from langchain_text_splitters import (\n",
    "    TokenTextSplitter, \n",
    "    RecursiveCharacterTextSplitter\n",
    ")\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.retrievers.document_compressors import DocumentCompressorPipeline\n",
    "from langchain_community.document_transformers import EmbeddingsRedundantFilter\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.retrievers.document_compressors import EmbeddingsFilter\n",
    "from langchain.chains import (\n",
    "    RetrievalQAWithSourcesChain, \n",
    "    RetrievalQA\n",
    ")\n",
    "from langchain.retrievers.document_compressors import LLMChainFilter\n",
    "from langchain_community.document_transformers import LongContextReorder\n",
    "from langchain.retrievers.merger_retriever import MergerRetriever\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "from langchain.chains import RetrievalQA\n",
    "import langchain\n",
    "langchain.debug = False\n",
    "\n",
    "\n",
    "load_dotenv(\n",
    "    find_dotenv()\n",
    ")\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "model_name = [\n",
    "    \"llama3-70b-8192\", \n",
    "    \"gemma2-9b-it\", \n",
    "    \"qwen-qwq-32b\"\n",
    "]\n",
    "\n",
    "llm_model = ChatGroq(\n",
    "    model=model_name[1], \n",
    "    temperature=0.4, \n",
    "    max_tokens=512, \n",
    "    model_kwargs={\n",
    "        \"top_p\" : 0.9,\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "def printDocuments(docs) :\n",
    "    print(\n",
    "        f\"\\n{'-' * 100}\\n\".join(\n",
    "            [\n",
    "                f\"Document {index + 1}:- \\n\\n\" + d.page_content for index, d in enumerate(docs)\n",
    "            ]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3bc3605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] ----> Running the Data Augmentation Pipeline for RAG, please wait....\n",
      "\n",
      "\n",
      "[INFO] ----> Total Pages in the original document  are :- 33\n",
      "[INFO] ----> Total Document Chunks Created are :- 293\n",
      "\n",
      "[INFO] ----> Using the Google-AI Embedding model...\n",
      "[INFO] ----> Google-AI Embedding model loaded.\n",
      "\n",
      "[INFO] ----> Creating the FAISS Vectorstore, please wait.....\n",
      "[INFO] ----> FAISS Retriever Created successfully.....\n",
      "\n",
      "[INFO] ----> Creating the Chroma Vectorstore, please wait.....\n",
      "[INFO] ----> Chroma Retriever Created successfully.....\n",
      "\n",
      "[INFO] ----> RAG Data Augmentation completed......\n"
     ]
    }
   ],
   "source": [
    "def DataAugmentationWithDualRetriever(\n",
    "    data_path, embedding_provider, chunk_size, chunk_overlap, \n",
    "    faiss_retriever_search_type, faiss_retriever_k_documents, \n",
    "    chroma_retriever_search_type, chroma_retriever_k_documents, **kwargs) :\n",
    "    if kwargs[\"execute_function\"] :\n",
    "        print(\"\\n[INFO] ----> Running the Data Augmentation Pipeline for RAG, please wait....\\n\")\n",
    "    \n",
    "        documents = PyMuPDFLoader(\n",
    "            data_path,\n",
    "        ).load()\n",
    "        print(\"\\n[INFO] ----> Total Pages in the original document  are :- {}\".format(len(documents)))\n",
    "\n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size, \n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=len,\n",
    "        )\n",
    "\n",
    "        document_chunks = splitter.split_documents(\n",
    "            documents\n",
    "        )\n",
    "\n",
    "        print(\"[INFO] ----> Total Document Chunks Created are :- {}\\n\".format(len(document_chunks)))\n",
    "\n",
    "        if embedding_provider == \"Google\" :\n",
    "            print(\"[INFO] ----> Using the Google-AI Embedding model...\")\n",
    "            embeddings = GoogleGenerativeAIEmbeddings(\n",
    "                model=\"models/embedding-001\"\n",
    "            )\n",
    "            print(\"[INFO] ----> Google-AI Embedding model loaded.\\n\")\n",
    "\n",
    "        elif embedding_provider == \"OpenAI\" :\n",
    "            print(\"[INFO] ----> Using the Open-AI Embedding model....\")\n",
    "            embeddings = OpenAIEmbeddings()\n",
    "            print(\"[INFO] ----> OpenAI Embedding model loaded.\\n\")\n",
    "\n",
    "        print(\"[INFO] ----> Creating the FAISS Vectorstore, please wait.....\")\n",
    "        faissRetriever = FAISS.from_documents(\n",
    "            document_chunks, embeddings).as_retriever(\n",
    "                search_type=faiss_retriever_search_type, \n",
    "                search_kwargs={\n",
    "                    \"k\" : faiss_retriever_k_documents\n",
    "                }\n",
    "            )\n",
    "        print(\"[INFO] ----> FAISS Retriever Created successfully.....\\n\")\n",
    "\n",
    "        print(\"[INFO] ----> Creating the Chroma Vectorstore, please wait.....\")\n",
    "        chromaRetriever = Chroma.from_documents(\n",
    "            document_chunks, \n",
    "            embeddings,\n",
    "        ).as_retriever(\n",
    "            search_type=chroma_retriever_search_type, \n",
    "            search_kwargs={\n",
    "                \"k\" : chroma_retriever_k_documents, \n",
    "            }\n",
    "        )\n",
    "        print(\"[INFO] ----> Chroma Retriever Created successfully.....\\n\")\n",
    "        print(\"[INFO] ----> RAG Data Augmentation completed......\")\n",
    "\n",
    "        return faissRetriever, chromaRetriever, embeddings\n",
    "    else :\n",
    "        print(\"[INFO] ----> Change the execute_function argument to True and then run...\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def EnsembleContextualCompressionRetriever(**kwargs) :\n",
    "    if kwargs[\"execute_pipeline\"] :\n",
    "        redundant_filter = EmbeddingsRedundantFilter(\n",
    "            embeddings=kwargs['embeddings'],\n",
    "        )\n",
    "        reordering = LongContextReorder()\n",
    "\n",
    "        relevant_filter_faiss = EmbeddingsFilter(\n",
    "            embeddings=kwargs['embeddings'], \n",
    "            similarity_threshold=kwargs['faiss_embeddingfilter_threshold'],\n",
    "        )\n",
    "        relevant_filter_chroma = EmbeddingsFilter(\n",
    "            embeddings=kwargs['embeddings'], \n",
    "            similarity_threshold=kwargs['chroma_embeddingfilter_threshold'],\n",
    "        )\n",
    "\n",
    "        document_pipeline_compressor_faiss = DocumentCompressorPipeline(\n",
    "            transformers=[\n",
    "                redundant_filter, \n",
    "                relevant_filter_faiss, \n",
    "                reordering,\n",
    "            ]\n",
    "        )\n",
    "        document_pipeline_compressor_chroma = DocumentCompressorPipeline(\n",
    "            transformers=[\n",
    "                redundant_filter, \n",
    "                relevant_filter_chroma,\n",
    "                reordering,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # First Compression Retriever with FAISS\n",
    "        compression_retriever_with_faiss = ContextualCompressionRetriever(\n",
    "            base_compressor=document_pipeline_compressor_faiss, \n",
    "            base_retriever=kwargs['faissRetriever'],\n",
    "\n",
    "        )\n",
    "        # Second Compression Retriever with Chroma\n",
    "        compression_retriever_with_chroma = ContextualCompressionRetriever(\n",
    "            base_compressor=document_pipeline_compressor_chroma, \n",
    "            base_retriever=kwargs['chromaRetriever'],\n",
    "        )\n",
    "\n",
    "        lotrCompression = MergerRetriever(\n",
    "            retrievers=[\n",
    "                compression_retriever_with_faiss, \n",
    "                compression_retriever_with_chroma\n",
    "            ]\n",
    "        )\n",
    "        return lotrCompression\n",
    "    else :\n",
    "        print(\"[INFO] ----> Change the execute_function argument to True and then run...\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def generateAnswerFunction(\n",
    "    question, llm_model, retriever_function, \n",
    "    verbose=-1, **kwargs) :\n",
    "    if kwargs[\"parse_function\"] :\n",
    "        rag_pipeline_type = kwargs[\"ragPipelineConfig\"]\n",
    "        if rag_pipeline_type == \"RetrievalQAWithSourcesChain\" :\n",
    "            print(\"[INFO] ----> Running the RetrievalQAWithSourcesChain Pipeline.....\\n\")\n",
    "            rag_pipeline = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "                llm = llm_model,\n",
    "                chain_type = kwargs[\"chain_type\"], \n",
    "                retriever = retriever_function,\n",
    "                return_source_documents=True\n",
    "            )\n",
    "            print(\"[INFO] ----> Generating the answer for:- \\nQuestion:- {}\".format(question))\n",
    "            generated_answer = rag_pipeline.invoke(question)\n",
    "            result = generated_answer[\"answer\"]\n",
    "            print(\"[INFO] ----> Result Answer generated.....\\n\")\n",
    "            if verbose > -1 :\n",
    "                print(\"Result :- {}{}\".format(\"\\n\", result))\n",
    "            return result\n",
    "\n",
    "        elif rag_pipeline_type == \"RetrievalQAChain\" :\n",
    "            print(\"[INFO] ----> Running the RetrievalQAChain Pipeline.....\\n\")\n",
    "            rag_pipeline = RetrievalQA.from_chain_type(\n",
    "                llm = llm_model,\n",
    "                chain_type = kwargs[\"chain_type\"], \n",
    "                retriever = retriever_function,\n",
    "                return_source_documents=True\n",
    "            )\n",
    "            print(\"[INFO] ----> Generating the answer for:- \\nQuestion:- {}\".format(question))\n",
    "            generated_answer = rag_pipeline.invoke(question)\n",
    "            result = generated_answer[\"result\"]\n",
    "            print(\"[INFO] ----> Result Answer generated.....\\n\")\n",
    "            if verbose > -1 :\n",
    "                print(\"Result :- {}{}\".format(\"\\n\", result))\n",
    "            return result\n",
    "\n",
    "\n",
    "\n",
    "dataPath = \"Data\\\\ReAct.pdf\"\n",
    "embedding = \"Google\"\n",
    "\n",
    "faissRetriever, chromaRetriever, embeddings = DataAugmentationWithDualRetriever(\n",
    "    data_path = dataPath,\n",
    "    embedding_provider = embedding,\n",
    "    chunk_size = 512,\n",
    "    chunk_overlap = 128,\n",
    "    faiss_retriever_search_type = \"similarity\",\n",
    "    faiss_retriever_k_documents = 3,\n",
    "    chroma_retriever_search_type = \"similarity\",\n",
    "    chroma_retriever_k_documents = 4,\n",
    "    execute_function = True\n",
    ")\n",
    "\n",
    "retriever = EnsembleContextualCompressionRetriever(\n",
    "    faiss_embeddingfilter_threshold = 0.75,\n",
    "    chroma_embeddingfilter_threshold = 0.6,\n",
    "    embeddings=embeddings, \n",
    "    execute_pipeline=True,\n",
    "    faissRetriever=faissRetriever, \n",
    "    chromaRetriever=chromaRetriever,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3395fc99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] ----> Running the RetrievalQAWithSourcesChain Pipeline.....\n",
      "\n",
      "[INFO] ----> Generating the answer for:- \n",
      "Question:- Explain in details about the Chain of thought prompting as mentioned in ReAct Paper.\n",
      "[INFO] ----> Result Answer generated.....\n",
      "\n",
      "\n",
      "\n",
      "============================================================================================================================================================================================================================================================================================================\n",
      "Final Answer :- \n",
      "Chain-of-thought prompting, as mentioned in the ReAct paper, involves providing the model with a sequence of thoughts leading to a solution. \n",
      "\n",
      "The paper describes it as a \"static black box\" because while the model uses chain-of-thought reasoning, the process is not transparent. We don't know exactly how the model arrives at each step in the chain.\n",
      "\n",
      "The ReAct paper proposes a new prompting method called \"ReAct\" that builds upon chain-of-thought prompting by incorporating both reasoning (thoughts) and actions taken by the model. This allows for a more dynamic and interpretable understanding of the model's decision-making process.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"Explain in details about the Chain of thought prompting as mentioned in ReAct Paper.\"\n",
    "\n",
    "answer = generateAnswerFunction(\n",
    "    question=query,\n",
    "    llm_model=llm_model, \n",
    "    retriever_function=retriever,\n",
    "    parse_function=True, \n",
    "    chain_type=\"stuff\",\n",
    "\n",
    "    # Choices are \"RetrievalQAWithSourcesChain\" or \"RetrievalQAChain\"\n",
    "    ragPipelineConfig=\"RetrievalQAWithSourcesChain\"\n",
    ")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"===\" * 100)\n",
    "print(\"Final Answer :- {}{}\".format(\"\\n\", answer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b29c87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hybrid-rag-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
