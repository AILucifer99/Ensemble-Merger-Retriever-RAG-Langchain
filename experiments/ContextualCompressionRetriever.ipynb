{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80d3e0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from langchain_text_splitters import (\n",
    "    TokenTextSplitter, \n",
    "    RecursiveCharacterTextSplitter\n",
    ")\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.retrievers.document_compressors import DocumentCompressorPipeline\n",
    "from langchain_community.document_transformers import EmbeddingsRedundantFilter\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.retrievers.document_compressors import EmbeddingsFilter\n",
    "from langchain.chains import (\n",
    "    RetrievalQAWithSourcesChain, \n",
    "    RetrievalQA\n",
    ")\n",
    "from langchain.retrievers.document_compressors import LLMChainFilter\n",
    "from langchain_community.document_transformers import LongContextReorder\n",
    "from langchain.retrievers.merger_retriever import MergerRetriever\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "from langchain.chains import RetrievalQA\n",
    "import langchain\n",
    "langchain.debug = False\n",
    "\n",
    "\n",
    "load_dotenv(\n",
    "    find_dotenv()\n",
    ")\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "model_name = [\n",
    "    \"llama3-70b-8192\", \n",
    "    \"gemma2-9b-it\", \n",
    "    \"qwen-qwq-32b\"\n",
    "]\n",
    "\n",
    "llm_model = ChatGroq(\n",
    "    model=model_name[1], \n",
    "    temperature=0.4, \n",
    "    max_tokens=512, \n",
    "    model_kwargs={\n",
    "        \"top_p\" : 0.9,\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "def printDocuments(docs) :\n",
    "    print(\n",
    "        f\"\\n{'-' * 100}\\n\".join(\n",
    "            [\n",
    "                f\"Document {index + 1}:- \\n\\n\" + d.page_content for index, d in enumerate(docs)\n",
    "            ]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bc3605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] ----> Running the Data Augmentation Pipeline for RAG, please wait....\n",
      "\n",
      "\n",
      "[INFO] ----> Total Pages in the original document  are :- 33\n",
      "[INFO] ----> Total Document Chunks Created are :- 293\n",
      "\n",
      "[INFO] ----> Using the Google-AI Embedding model...\n",
      "[INFO] ----> Google-AI Embedding model loaded.\n",
      "\n",
      "[INFO] ----> Creating the FAISS Vectorstore, please wait.....\n",
      "[INFO] ----> FAISS Retriever Created successfully.....\n",
      "\n",
      "[INFO] ----> Creating the Chroma Vectorstore, please wait.....\n",
      "[INFO] ----> Chroma Retriever Created successfully.....\n",
      "\n",
      "[INFO] ----> RAG Data Augmentation completed......\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from langchain_text_splitters import (\n",
    "    TokenTextSplitter, \n",
    "    RecursiveCharacterTextSplitter\n",
    ")\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.retrievers.document_compressors import DocumentCompressorPipeline\n",
    "from langchain_community.document_transformers import EmbeddingsRedundantFilter\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.retrievers.document_compressors import EmbeddingsFilter\n",
    "from langchain.chains import (\n",
    "    RetrievalQAWithSourcesChain, \n",
    "    RetrievalQA\n",
    ")\n",
    "from langchain.retrievers.document_compressors import LLMChainFilter\n",
    "from langchain_community.document_transformers import LongContextReorder\n",
    "from langchain.retrievers.merger_retriever import MergerRetriever\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "from langchain.chains import RetrievalQA\n",
    "import langchain\n",
    "langchain.debug = False\n",
    "\n",
    "\n",
    "load_dotenv(\n",
    "    find_dotenv()\n",
    ")\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "model_name = [\n",
    "    \"llama3-70b-8192\", \n",
    "    \"gemma2-9b-it\", \n",
    "    \"qwen-qwq-32b\"\n",
    "]\n",
    "\n",
    "llm_model = ChatGroq(\n",
    "    model=model_name[1], \n",
    "    temperature=0.4, \n",
    "    max_tokens=512, \n",
    "    model_kwargs={\n",
    "        \"top_p\" : 0.9,\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "def printDocuments(docs) :\n",
    "    print(\n",
    "        f\"\\n{'-' * 100}\\n\".join(\n",
    "            [\n",
    "                f\"Document {index + 1}:- \\n\\n\" + d.page_content for index, d in enumerate(docs)\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def DataAugmentationWithDualRetriever(\n",
    "    data_path, embedding_provider, chunk_size, chunk_overlap, \n",
    "    faiss_retriever_search_type, faiss_retriever_k_documents, \n",
    "    chroma_retriever_search_type, chroma_retriever_k_documents, **kwargs) :\n",
    "    if kwargs[\"execute_function\"] :\n",
    "        print(\"\\n[INFO] ----> Running the Data Augmentation Pipeline for RAG, please wait....\\n\")\n",
    "    \n",
    "        documents = PyMuPDFLoader(\n",
    "            data_path,\n",
    "        ).load()\n",
    "        print(\"\\n[INFO] ----> Total Pages in the original document  are :- {}\".format(len(documents)))\n",
    "\n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size, \n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=len,\n",
    "        )\n",
    "\n",
    "        document_chunks = splitter.split_documents(\n",
    "            documents\n",
    "        )\n",
    "\n",
    "        print(\"[INFO] ----> Total Document Chunks Created are :- {}\\n\".format(len(document_chunks)))\n",
    "\n",
    "        if embedding_provider == \"Google\" :\n",
    "            print(\"[INFO] ----> Using the Google-AI Embedding model...\")\n",
    "            embeddings = GoogleGenerativeAIEmbeddings(\n",
    "                model=\"models/embedding-001\"\n",
    "            )\n",
    "            print(\"[INFO] ----> Google-AI Embedding model loaded.\\n\")\n",
    "\n",
    "        elif embedding_provider == \"OpenAI\" :\n",
    "            print(\"[INFO] ----> Using the Open-AI Embedding model....\")\n",
    "            embeddings = OpenAIEmbeddings()\n",
    "            print(\"[INFO] ----> OpenAI Embedding model loaded.\\n\")\n",
    "\n",
    "        print(\"[INFO] ----> Creating the FAISS Vectorstore, please wait.....\")\n",
    "        faissRetriever = FAISS.from_documents(\n",
    "            document_chunks, embeddings).as_retriever(\n",
    "                search_type=faiss_retriever_search_type, \n",
    "                search_kwargs={\n",
    "                    \"k\" : faiss_retriever_k_documents\n",
    "                }\n",
    "            )\n",
    "        print(\"[INFO] ----> FAISS Retriever Created successfully.....\\n\")\n",
    "\n",
    "        print(\"[INFO] ----> Creating the Chroma Vectorstore, please wait.....\")\n",
    "        chromaRetriever = Chroma.from_documents(\n",
    "            document_chunks, \n",
    "            embeddings,\n",
    "        ).as_retriever(\n",
    "            search_type=chroma_retriever_search_type, \n",
    "            search_kwargs={\n",
    "                \"k\" : chroma_retriever_k_documents, \n",
    "            }\n",
    "        )\n",
    "        print(\"[INFO] ----> Chroma Retriever Created successfully.....\\n\")\n",
    "        print(\"[INFO] ----> RAG Data Augmentation completed......\")\n",
    "\n",
    "        return faissRetriever, chromaRetriever, embeddings\n",
    "    else :\n",
    "        print(\"[INFO] ----> Change the execute_function argument to True and then run...\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def EnsembleContextualCompressionRetriever(**kwargs) :\n",
    "    if kwargs[\"execute_pipeline\"] :\n",
    "        redundant_filter = EmbeddingsRedundantFilter(\n",
    "            embeddings=kwargs['embeddings'],\n",
    "        )\n",
    "        reordering = LongContextReorder()\n",
    "\n",
    "        relevant_filter_faiss = EmbeddingsFilter(\n",
    "            embeddings=kwargs['embeddings'], \n",
    "            similarity_threshold=kwargs['faiss_embeddingfilter_threshold'],\n",
    "        )\n",
    "        relevant_filter_chroma = EmbeddingsFilter(\n",
    "            embeddings=kwargs['embeddings'], \n",
    "            similarity_threshold=kwargs['chroma_embeddingfilter_threshold'],\n",
    "        )\n",
    "\n",
    "        document_pipeline_compressor_faiss = DocumentCompressorPipeline(\n",
    "            transformers=[\n",
    "                redundant_filter, \n",
    "                relevant_filter_faiss, \n",
    "                reordering,\n",
    "            ]\n",
    "        )\n",
    "        document_pipeline_compressor_chroma = DocumentCompressorPipeline(\n",
    "            transformers=[\n",
    "                redundant_filter, \n",
    "                relevant_filter_chroma,\n",
    "                reordering,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # First Compression Retriever with FAISS\n",
    "        compression_retriever_with_faiss = ContextualCompressionRetriever(\n",
    "            base_compressor=document_pipeline_compressor_faiss, \n",
    "            base_retriever=kwargs['faissRetriever'],\n",
    "\n",
    "        )\n",
    "        # Second Compression Retriever with Chroma\n",
    "        compression_retriever_with_chroma = ContextualCompressionRetriever(\n",
    "            base_compressor=document_pipeline_compressor_chroma, \n",
    "            base_retriever=kwargs['chromaRetriever'],\n",
    "        )\n",
    "\n",
    "        lotrCompression = MergerRetriever(\n",
    "            retrievers=[\n",
    "                compression_retriever_with_faiss, \n",
    "                compression_retriever_with_chroma\n",
    "            ]\n",
    "        )\n",
    "        return lotrCompression\n",
    "    else :\n",
    "        print(\"[INFO] ----> Change the execute_function argument to True and then run...\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def generateAnswerFunction(\n",
    "    question, llm_model, retriever_function, \n",
    "    verbose=-1, **kwargs) :\n",
    "    if kwargs[\"parse_function\"] :\n",
    "        rag_pipeline_type = kwargs[\"ragPipelineConfig\"]\n",
    "        if rag_pipeline_type == \"RetrievalQAWithSourcesChain\" :\n",
    "            print(\"[INFO] ----> Running the RetrievalQAWithSourcesChain Pipeline.....\\n\")\n",
    "            rag_pipeline = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "                llm = llm_model,\n",
    "                chain_type = kwargs[\"chain_type\"], \n",
    "                retriever = retriever_function,\n",
    "                return_source_documents=True\n",
    "            )\n",
    "            print(\"[INFO] ----> Generating the answer for:- \\nQuestion:- {}\".format(question))\n",
    "            generated_answer = rag_pipeline.invoke(question)\n",
    "            result = generated_answer[\"answer\"]\n",
    "            print(\"[INFO] ----> Result Answer generated.....\\n\")\n",
    "            if verbose > -1 :\n",
    "                print(\"Result :- {}{}\".format(\"\\n\", result))\n",
    "            return result\n",
    "\n",
    "        elif rag_pipeline_type == \"RetrievalQAChain\" :\n",
    "            print(\"[INFO] ----> Running the RetrievalQAChain Pipeline.....\\n\")\n",
    "            rag_pipeline = RetrievalQA.from_chain_type(\n",
    "                llm = llm_model,\n",
    "                chain_type = kwargs[\"chain_type\"], \n",
    "                retriever = retriever_function,\n",
    "                return_source_documents=True\n",
    "            )\n",
    "            print(\"[INFO] ----> Generating the answer for:- \\nQuestion:- {}\".format(question))\n",
    "            generated_answer = rag_pipeline.invoke(question)\n",
    "            result = generated_answer[\"result\"]\n",
    "            print(\"[INFO] ----> Result Answer generated.....\\n\")\n",
    "            if verbose > -1 :\n",
    "                print(\"Result :- {}{}\".format(\"\\n\", result))\n",
    "            return result\n",
    "\n",
    "\n",
    "\n",
    "dataPath = \"Data\\\\ReAct.pdf\"\n",
    "embedding = \"Google\"\n",
    "\n",
    "faissRetriever, chromaRetriever, embeddings = DataAugmentationWithDualRetriever(\n",
    "    data_path = dataPath,\n",
    "    embedding_provider = embedding,\n",
    "    chunk_size = 512,\n",
    "    chunk_overlap = 128,\n",
    "    faiss_retriever_search_type = \"similarity\",\n",
    "    faiss_retriever_k_documents = 3,\n",
    "    chroma_retriever_search_type = \"similarity\",\n",
    "    chroma_retriever_k_documents = 4,\n",
    "    execute_function = True\n",
    ")\n",
    "\n",
    "retriever = EnsembleContextualCompressionRetriever(\n",
    "    faiss_embeddingfilter_threshold = 0.75,\n",
    "    chroma_embeddingfilter_threshold = 0.6,\n",
    "    embeddings=embeddings, \n",
    "    execute_pipeline=True,\n",
    "    faissRetriever=faissRetriever, \n",
    "    chromaRetriever=chromaRetriever,\n",
    ")\n",
    "\n",
    "\n",
    "query = \"Explain in details about the Chain of thought prompting as mentioned in ReAct Paper.\"\n",
    "\n",
    "answer = generateAnswerFunction(\n",
    "    question=query,\n",
    "    llm_model=llm_model, \n",
    "    retriever_function=retriever,\n",
    "    parse_function=True, \n",
    "    chain_type=\"stuff\",\n",
    "\n",
    "    # Choices are \"RetrievalQAWithSourcesChain\" or \"RetrievalQAChain\"\n",
    "    ragPipelineConfig=\"RetrievalQAWithSourcesChain\"\n",
    ")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"===\" * 100)\n",
    "print(\"Final Answer :- {}{}\".format(\"\\n\", answer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3395fc99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] ----> Running the RetrievalQAWithSourcesChain Pipeline.....\n",
      "\n",
      "[INFO] ----> Generating the answer for:- \n",
      "Question:- Explain in details about the Chain of thought prompting as mentioned in ReAct Paper.\n",
      "[INFO] ----> Result Answer generated.....\n",
      "\n",
      "\n",
      "\n",
      "============================================================================================================================================================================================================================================================================================================\n",
      "Final Answer :- \n",
      "Chain-of-thought prompting, as mentioned in the ReAct paper, involves providing the model with a sequence of thoughts leading to a solution. \n",
      "\n",
      "The paper describes it as a \"static black box\" because while the model uses chain-of-thought reasoning, the process is not transparent. We don't know exactly how the model arrives at each step in the chain.\n",
      "\n",
      "The ReAct paper proposes a new prompting method called \"ReAct\" that builds upon chain-of-thought prompting by incorporating both reasoning (thoughts) and actions taken by the model. This allows for a more dynamic and interpretable understanding of the model's decision-making process.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"Explain in details about the Chain of thought prompting as mentioned in ReAct Paper.\"\n",
    "\n",
    "answer = generateAnswerFunction(\n",
    "    question=query,\n",
    "    llm_model=llm_model, \n",
    "    retriever_function=retriever,\n",
    "    parse_function=True, \n",
    "    chain_type=\"stuff\",\n",
    "\n",
    "    # Choices are \"RetrievalQAWithSourcesChain\" or \"RetrievalQAChain\"\n",
    "    ragPipelineConfig=\"RetrievalQAWithSourcesChain\"\n",
    ")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"===\" * 100)\n",
    "print(\"Final Answer :- {}{}\".format(\"\\n\", answer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16b29c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] ----> Loading and splitting the document, please wait....\n",
      "\n",
      "\n",
      "[INFO] ----> Total Pages in the original document are: 33\n",
      "[INFO] ----> Total Document Chunks Created are: 293\n",
      "\n",
      "[INFO] ----> Using the Google-AI Embedding model...\n",
      "[INFO] ----> Google-AI Embedding model loaded.\n",
      "\n",
      "[INFO] ----> Creating the FAISS Vectorstore, please wait.....\n",
      "[INFO] ----> FAISS Retriever Created successfully.....\n",
      "\n",
      "[INFO] ----> Creating the Chroma Vectorstore, please wait.....\n",
      "[INFO] ----> Chroma Retriever Created successfully.....\n",
      "\n",
      "[INFO] ----> Building the ensemble merger retriever, please wait.....\n",
      "[INFO] ----> Ensemble merger retriever built successfully.....\n",
      "[INFO] ----> Running the RetrievalQAWithSourcesChain Pipeline.....\n",
      "\n",
      "[INFO] ----> Generating the answer for:\n",
      "Question: Explain in details about the Chain of thought prompting as mentioned in ReAct Paper.\n",
      "[INFO] ----> Result Answer generated.....\n",
      "\n",
      "Result:\n",
      "The ReAct paper proposes a new prompting method called ReAct that encourages models to generate a sequence of thoughts and actions when solving a task. \n",
      "\n",
      "This is in contrast to other methods like \"Chain-of-thought\" prompting (CoT), which only focuses on the thought process and ignores the actions taken. \n",
      "\n",
      "ReAct's key features are:\n",
      "\n",
      "* **Reason+Act:** It explicitly includes both reasoning (thoughts) and actions in the prompt, allowing the model to reason about its actions and their consequences.\n",
      "* **Flexible Thought Space:** ReAct doesn't impose a rigid format on the thoughts generated, allowing for more natural and diverse reasoning patterns.\n",
      "* **General and Flexible:** ReAct can be applied to a wide range of tasks with different action spaces and reasoning needs.\n",
      "\n",
      "The paper argues that this approach leads to improved performance compared to other prompting methods, as it provides the model with a richer and more informative context for solving the task.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "============================================================================================================================================================================================================================================================================================================\n",
      "Final Answer:\n",
      "The ReAct paper proposes a new prompting method called ReAct that encourages models to generate a sequence of thoughts and actions when solving a task. \n",
      "\n",
      "This is in contrast to other methods like \"Chain-of-thought\" prompting (CoT), which only focuses on the thought process and ignores the actions taken. \n",
      "\n",
      "ReAct's key features are:\n",
      "\n",
      "* **Reason+Act:** It explicitly includes both reasoning (thoughts) and actions in the prompt, allowing the model to reason about its actions and their consequences.\n",
      "* **Flexible Thought Space:** ReAct doesn't impose a rigid format on the thoughts generated, allowing for more natural and diverse reasoning patterns.\n",
      "* **General and Flexible:** ReAct can be applied to a wide range of tasks with different action spaces and reasoning needs.\n",
      "\n",
      "The paper argues that this approach leads to improved performance compared to other prompting methods, as it provides the model with a richer and more informative context for solving the task.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.retrievers.document_compressors import DocumentCompressorPipeline\n",
    "from langchain_community.document_transformers import EmbeddingsRedundantFilter\n",
    "from langchain.retrievers.document_compressors import EmbeddingsFilter\n",
    "from langchain_community.document_transformers import LongContextReorder\n",
    "from langchain.retrievers.merger_retriever import MergerRetriever\n",
    "from langchain.chains import RetrievalQAWithSourcesChain, RetrievalQA\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "\n",
    "\n",
    "class EnsembleMergerRetriever:\n",
    "    def __init__(self, load_env=True):\n",
    "        \"\"\"\n",
    "        Initialize the EnsembleMergerRetriever class.\n",
    "        \n",
    "        Args:\n",
    "            load_env (bool): Whether to load environment variables from .env file\n",
    "        \"\"\"\n",
    "        if load_env:\n",
    "            load_dotenv(find_dotenv())\n",
    "            os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")\n",
    "            os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "        \n",
    "        self.faiss_retriever = None\n",
    "        self.chroma_retriever = None\n",
    "        self.embeddings = None\n",
    "        self.merger_retriever = None\n",
    "        self.llm_model = None\n",
    "        self.documents = None\n",
    "        self.document_chunks = None\n",
    "    \n",
    "    def setup_llm(self, model_name=\"gemma2-9b-it\", temperature=0.4, max_tokens=512, top_p=0.9):\n",
    "        \"\"\"\n",
    "        Set up the LLM model.\n",
    "        \n",
    "        Args:\n",
    "            model_name (str): Model name (options: \"llama3-70b-8192\", \"gemma2-9b-it\", \"qwen-qwq-32b\")\n",
    "            temperature (float): Temperature for generation\n",
    "            max_tokens (int): Maximum tokens to generate\n",
    "            top_p (float): Top-p value for generation\n",
    "            \n",
    "        Returns:\n",
    "            EnsembleMergerRetriever: self instance for method chaining\n",
    "        \"\"\"\n",
    "        self.llm_model = ChatGroq(\n",
    "            model=model_name,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "            model_kwargs={\n",
    "                \"top_p\": top_p,\n",
    "            },\n",
    "        )\n",
    "        return self\n",
    "    \n",
    "    def load_and_split_documents(self, data_path, chunk_size=512, chunk_overlap=128):\n",
    "        \"\"\"\n",
    "        Load and split documents for retrieval.\n",
    "        \n",
    "        Args:\n",
    "            data_path (str): Path to the PDF document\n",
    "            chunk_size (int): Size of chunks for splitting\n",
    "            chunk_overlap (int): Overlap between chunks\n",
    "            \n",
    "        Returns:\n",
    "            EnsembleMergerRetriever: self instance for method chaining\n",
    "        \"\"\"\n",
    "        print(\"\\n[INFO] ----> Loading and splitting the document, please wait....\\n\")\n",
    "        \n",
    "        # Load documents\n",
    "        self.documents = PyMuPDFLoader(data_path).load()\n",
    "        print(\"\\n[INFO] ----> Total Pages in the original document are: {}\".format(len(self.documents)))\n",
    "        \n",
    "        # Split documents\n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=len,\n",
    "        )\n",
    "        \n",
    "        self.document_chunks = splitter.split_documents(self.documents)\n",
    "        print(\"[INFO] ----> Total Document Chunks Created are: {}\\n\".format(len(self.document_chunks)))\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def setup_embeddings(self, embedding_provider=\"Google\"):\n",
    "        \"\"\"\n",
    "        Set up embeddings for vectorstores.\n",
    "        \n",
    "        Args:\n",
    "            embedding_provider (str): Provider for embeddings (\"Google\" or \"OpenAI\")\n",
    "            \n",
    "        Returns:\n",
    "            EnsembleMergerRetriever: self instance for method chaining\n",
    "        \"\"\"\n",
    "        if embedding_provider == \"Google\":\n",
    "            print(\"[INFO] ----> Using the Google-AI Embedding model...\")\n",
    "            self.embeddings = GoogleGenerativeAIEmbeddings(\n",
    "                model=\"models/embedding-001\"\n",
    "            )\n",
    "            print(\"[INFO] ----> Google-AI Embedding model loaded.\\n\")\n",
    "        \n",
    "        elif embedding_provider == \"OpenAI\":\n",
    "            print(\"[INFO] ----> Using the Open-AI Embedding model...\")\n",
    "            self.embeddings = OpenAIEmbeddings()\n",
    "            print(\"[INFO] ----> OpenAI Embedding model loaded.\\n\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def create_retrievers(self, \n",
    "                         faiss_search_type=\"similarity\", \n",
    "                         faiss_k_documents=3,\n",
    "                         chroma_search_type=\"similarity\", \n",
    "                         chroma_k_documents=4):\n",
    "        \"\"\"\n",
    "        Create FAISS and Chroma retrievers.\n",
    "        \n",
    "        Args:\n",
    "            faiss_search_type (str): Search type for FAISS retriever\n",
    "            faiss_k_documents (int): Number of documents to retrieve with FAISS\n",
    "            chroma_search_type (str): Search type for Chroma retriever\n",
    "            chroma_k_documents (int): Number of documents to retrieve with Chroma\n",
    "            \n",
    "        Returns:\n",
    "            EnsembleMergerRetriever: self instance for method chaining\n",
    "        \"\"\"\n",
    "        if self.document_chunks is None or self.embeddings is None:\n",
    "            print(\"[ERROR] ----> Documents and embeddings must be set up first\")\n",
    "            return self\n",
    "            \n",
    "        # Create FAISS retriever\n",
    "        print(\"[INFO] ----> Creating the FAISS Vectorstore, please wait.....\")\n",
    "        self.faiss_retriever = FAISS.from_documents(\n",
    "            self.document_chunks, \n",
    "            self.embeddings\n",
    "        ).as_retriever(\n",
    "            search_type=faiss_search_type,\n",
    "            search_kwargs={\n",
    "                \"k\": faiss_k_documents\n",
    "            }\n",
    "        )\n",
    "        print(\"[INFO] ----> FAISS Retriever Created successfully.....\\n\")\n",
    "        \n",
    "        # Create Chroma retriever\n",
    "        print(\"[INFO] ----> Creating the Chroma Vectorstore, please wait.....\")\n",
    "        self.chroma_retriever = Chroma.from_documents(\n",
    "            self.document_chunks,\n",
    "            self.embeddings,\n",
    "        ).as_retriever(\n",
    "            search_type=chroma_search_type,\n",
    "            search_kwargs={\n",
    "                \"k\": chroma_k_documents,\n",
    "            }\n",
    "        )\n",
    "        print(\"[INFO] ----> Chroma Retriever Created successfully.....\\n\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def build_merger_retriever(self, \n",
    "                              faiss_embedding_filter_threshold=0.75,\n",
    "                              chroma_embedding_filter_threshold=0.6):\n",
    "        \"\"\"\n",
    "        Build the ensemble merger retriever with contextual compression.\n",
    "        \n",
    "        Args:\n",
    "            faiss_embedding_filter_threshold (float): Similarity threshold for FAISS embeddings filter\n",
    "            chroma_embedding_filter_threshold (float): Similarity threshold for Chroma embeddings filter\n",
    "            \n",
    "        Returns:\n",
    "            EnsembleMergerRetriever: self instance for method chaining\n",
    "        \"\"\"\n",
    "        if self.faiss_retriever is None or self.chroma_retriever is None:\n",
    "            print(\"[ERROR] ----> FAISS and Chroma retrievers must be created first\")\n",
    "            return self\n",
    "            \n",
    "        print(\"[INFO] ----> Building the ensemble merger retriever, please wait.....\")\n",
    "        \n",
    "        # Create shared filters\n",
    "        redundant_filter = EmbeddingsRedundantFilter(embeddings=self.embeddings)\n",
    "        reordering = LongContextReorder()\n",
    "        \n",
    "        # Create filters for FAISS\n",
    "        relevant_filter_faiss = EmbeddingsFilter(\n",
    "            embeddings=self.embeddings,\n",
    "            similarity_threshold=faiss_embedding_filter_threshold,\n",
    "        )\n",
    "        \n",
    "        # Create filters for Chroma\n",
    "        relevant_filter_chroma = EmbeddingsFilter(\n",
    "            embeddings=self.embeddings,\n",
    "            similarity_threshold=chroma_embedding_filter_threshold,\n",
    "        )\n",
    "        \n",
    "        # Create compressor pipelines\n",
    "        document_pipeline_compressor_faiss = DocumentCompressorPipeline(\n",
    "            transformers=[\n",
    "                redundant_filter,\n",
    "                relevant_filter_faiss,\n",
    "                reordering,\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        document_pipeline_compressor_chroma = DocumentCompressorPipeline(\n",
    "            transformers=[\n",
    "                redundant_filter,\n",
    "                relevant_filter_chroma,\n",
    "                reordering,\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Create compression retrievers\n",
    "        compression_retriever_with_faiss = ContextualCompressionRetriever(\n",
    "            base_compressor=document_pipeline_compressor_faiss,\n",
    "            base_retriever=self.faiss_retriever,\n",
    "        )\n",
    "        \n",
    "        compression_retriever_with_chroma = ContextualCompressionRetriever(\n",
    "            base_compressor=document_pipeline_compressor_chroma,\n",
    "            base_retriever=self.chroma_retriever,\n",
    "        )\n",
    "        \n",
    "        # Create merger retriever\n",
    "        self.merger_retriever = MergerRetriever(\n",
    "            retrievers=[\n",
    "                compression_retriever_with_faiss,\n",
    "                compression_retriever_with_chroma\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        print(\"[INFO] ----> Ensemble merger retriever built successfully.....\")\n",
    "        return self\n",
    "    \n",
    "    def generate_answer(self, question, pipeline_type=\"RetrievalQAWithSourcesChain\", chain_type=\"stuff\", verbose=False):\n",
    "        \"\"\"\n",
    "        Generate answer using the RAG pipeline.\n",
    "        \n",
    "        Args:\n",
    "            question (str): Question to answer\n",
    "            pipeline_type (str): Type of RAG pipeline (\"RetrievalQAWithSourcesChain\" or \"RetrievalQAChain\")\n",
    "            chain_type (str): Type of chain for retrieval QA\n",
    "            verbose (bool): Whether to print the result\n",
    "            \n",
    "        Returns:\n",
    "            str: Generated answer\n",
    "        \"\"\"\n",
    "        if self.merger_retriever is None or self.llm_model is None:\n",
    "            print(\"[ERROR] ----> Merger retriever and LLM must be set up first\")\n",
    "            return None\n",
    "            \n",
    "        print(f\"[INFO] ----> Running the {pipeline_type} Pipeline.....\\n\")\n",
    "        \n",
    "        if pipeline_type == \"RetrievalQAWithSourcesChain\":\n",
    "            rag_pipeline = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "                llm=self.llm_model,\n",
    "                chain_type=chain_type,\n",
    "                retriever=self.merger_retriever,\n",
    "                return_source_documents=True\n",
    "            )\n",
    "            print(f\"[INFO] ----> Generating the answer for:\\nQuestion: {question}\")\n",
    "            generated_answer = rag_pipeline.invoke(question)\n",
    "            result = generated_answer[\"answer\"]\n",
    "            \n",
    "        elif pipeline_type == \"RetrievalQAChain\":\n",
    "            rag_pipeline = RetrievalQA.from_chain_type(\n",
    "                llm=self.llm_model,\n",
    "                chain_type=chain_type,\n",
    "                retriever=self.merger_retriever,\n",
    "                return_source_documents=True\n",
    "            )\n",
    "            print(f\"[INFO] ----> Generating the answer for:\\nQuestion: {question}\")\n",
    "            generated_answer = rag_pipeline.invoke(question)\n",
    "            result = generated_answer[\"result\"]\n",
    "        \n",
    "        print(\"[INFO] ----> Result Answer generated.....\\n\")\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Result:\\n{result}\")\n",
    "            \n",
    "        return result\n",
    "    \n",
    "    def print_documents(self, docs):\n",
    "        \"\"\"\n",
    "        Print documents for inspection.\n",
    "        \n",
    "        Args:\n",
    "            docs (list): List of documents to print\n",
    "        \"\"\"\n",
    "        print(\n",
    "            f\"\\n{'-' * 100}\\n\".join(\n",
    "                [\n",
    "                    f\"Document {index + 1}:- \\n\\n\" + d.page_content for index, d in enumerate(docs)\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Create the ensemble retriever\n",
    "    retriever = EnsembleMergerRetriever()\n",
    "    \n",
    "    # Set up the LLM model\n",
    "    retriever.setup_llm(model_name=\"gemma2-9b-it\")\n",
    "    \n",
    "    # Load and split documents\n",
    "    retriever.load_and_split_documents(\n",
    "        data_path=\"Data\\\\ReAct.pdf\",\n",
    "        chunk_size=512,\n",
    "        chunk_overlap=128\n",
    "    )\n",
    "    \n",
    "    # Set up embeddings\n",
    "    retriever.setup_embeddings(embedding_provider=\"Google\")\n",
    "    \n",
    "    # Create retrievers\n",
    "    retriever.create_retrievers(\n",
    "        faiss_search_type=\"similarity\",\n",
    "        faiss_k_documents=3,\n",
    "        chroma_search_type=\"similarity\",\n",
    "        chroma_k_documents=4\n",
    "    )\n",
    "    \n",
    "    # Build the merger retriever\n",
    "    retriever.build_merger_retriever(\n",
    "        faiss_embedding_filter_threshold=0.75,\n",
    "        chroma_embedding_filter_threshold=0.6\n",
    "    )\n",
    "    \n",
    "    # Generate answer\n",
    "    query = \"Explain in details about the Chain of thought prompting as mentioned in ReAct Paper.\"\n",
    "    answer = retriever.generate_answer(\n",
    "        question=query,\n",
    "        pipeline_type=\"RetrievalQAWithSourcesChain\",\n",
    "        chain_type=\"stuff\",\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(\"===\" * 100)\n",
    "    print(f\"Final Answer:\\n{answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42df37a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] ----> Running the RetrievalQAWithSourcesChain Pipeline.....\n",
      "\n",
      "[INFO] ----> Generating the answer for:\n",
      "Question: Explain about LLMs with reference to the ReAct Algorithm\n",
      "[INFO] ----> Result Answer generated.....\n",
      "\n",
      "Result:\n",
      "ReAct is a paradigm that combines reasoning and acting with language models (LLMs) to solve diverse language reasoning and decision-making tasks. \n",
      "\n",
      "Unlike other methods that rely on expensive datasets and human feedback for policy learning, ReAct learns a policy in a more cost-effective way by prompting LLMs to generate both verbal reasoning traces and actions. \n",
      "\n",
      "This approach is inspired by the observation that humans often verbalize their reasoning process before taking action. \n",
      "\n",
      "ReAct's key contributions include:\n",
      "\n",
      "* Demonstrating the feasibility of combining reasoning and action with LLMs in an interactive environment within a closed-loop system.\n",
      "* Exploring the value of internal reasoning versus external feedback for improving task performance.\n",
      "* Providing a general framework for integrating reasoning and acting capabilities into LLMs, which can be applied to a wide range of tasks.\n",
      "\n",
      "The paper also highlights the limitations of current LLMs, noting that they still fall short of human performance in terms of product exploration and query reformulation.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"Explain about LLMs with reference to the ReAct Algorithm\"\n",
    "\n",
    "answer = retriever.generate_answer(\n",
    "    question=query,\n",
    "    pipeline_type=\"RetrievalQAWithSourcesChain\",\n",
    "    chain_type=\"stuff\",\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a3affb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] ----> Loading and splitting the document, please wait....\n",
      "\n",
      "\n",
      "[INFO] ----> Total Pages in the original document are: 15\n",
      "[INFO] ----> Total Document Chunks Created are: 108\n",
      "\n",
      "[INFO] ----> Using the Google-AI Embedding model...\n",
      "[INFO] ----> Google-AI Embedding model loaded.\n",
      "\n",
      "[INFO] ----> Creating the FAISS Vectorstore, please wait.....\n",
      "[INFO] ----> FAISS Retriever Created successfully.....\n",
      "\n",
      "[INFO] ----> Creating the Chroma Vectorstore, please wait.....\n",
      "[INFO] ----> Chroma Retriever Created successfully.....\n",
      "\n",
      "[INFO] ----> Building the ensemble merger retriever, please wait.....\n",
      "[INFO] ----> Ensemble merger retriever built successfully.....\n",
      "[INFO] ----> Running the RetrievalQAWithSourcesChain Pipeline.....\n",
      "\n",
      "[INFO] ----> Generating the answer for:\n",
      "Question: Explain the concept of Self-Attention mechanism\n",
      "[INFO] ----> Result Answer generated.....\n",
      "\n"
     ]
    }
   ],
   "source": [
    "retriever = EnsembleMergerRetriever()\n",
    "\n",
    "retriever.setup_llm() \\\n",
    "    .load_and_split_documents(\"Data\\\\Attention.pdf\") \\\n",
    "    .setup_embeddings() \\\n",
    "    .create_retrievers() \\\n",
    "    .build_merger_retriever()\n",
    "\n",
    "\n",
    "answer = retriever.generate_answer(\"Explain the concept of Self-Attention mechanism\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0116207d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-attention is an attention mechanism that relates different positions of a single sequence to compute a representation of the sequence. \n",
      "\n",
      "It allows the model to weigh the importance of different words in a sentence when understanding its meaning. \n",
      "\n",
      "Self-attention has been successfully used in various natural language processing tasks, such as reading comprehension and abstractive summarization.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdac8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.retrievers.document_compressors import DocumentCompressorPipeline\n",
    "from langchain_community.document_transformers import EmbeddingsRedundantFilter\n",
    "from langchain.retrievers.document_compressors import EmbeddingsFilter\n",
    "from langchain_community.document_transformers import LongContextReorder\n",
    "from langchain.retrievers.merger_retriever import MergerRetriever\n",
    "from langchain.chains import RetrievalQAWithSourcesChain, RetrievalQA\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "import asyncio\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "\n",
    "class AsyncEnsembleMergerRetriever:\n",
    "    def __init__(self, load_env=True):\n",
    "        \"\"\"\n",
    "        Initialize the AsyncEnsembleMergerRetriever class.\n",
    "        \n",
    "        Args:\n",
    "            load_env (bool): Whether to load environment variables from .env file\n",
    "        \"\"\"\n",
    "        if load_env:\n",
    "            load_dotenv(find_dotenv())\n",
    "            os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")\n",
    "            os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "        \n",
    "        self.faiss_retriever = None\n",
    "        self.chroma_retriever = None\n",
    "        self.embeddings = None\n",
    "        self.merger_retriever = None\n",
    "        self.llm_model = None\n",
    "        self.documents = None\n",
    "        self.document_chunks = None\n",
    "    \n",
    "    async def setup_llm(self, model_name=\"gemma2-9b-it\", temperature=0.4, max_tokens=512, top_p=0.9):\n",
    "        \"\"\"\n",
    "        Set up the LLM model asynchronously.\n",
    "        \n",
    "        Args:\n",
    "            model_name (str): Model name (options: \"llama3-70b-8192\", \"gemma2-9b-it\", \"qwen-qwq-32b\")\n",
    "            temperature (float): Temperature for generation\n",
    "            max_tokens (int): Maximum tokens to generate\n",
    "            top_p (float): Top-p value for generation\n",
    "            \n",
    "        Returns:\n",
    "            AsyncEnsembleMergerRetriever: self instance for method chaining\n",
    "        \"\"\"\n",
    "        # Create the LLM model\n",
    "        self.llm_model = ChatGroq(\n",
    "            model=model_name,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "            model_kwargs={\n",
    "                \"top_p\": top_p,\n",
    "            },\n",
    "        )\n",
    "        return self\n",
    "    \n",
    "    async def load_and_split_documents(self, data_path, chunk_size=512, chunk_overlap=128):\n",
    "        \"\"\"\n",
    "        Load and split documents for retrieval asynchronously.\n",
    "        \n",
    "        Args:\n",
    "            data_path (str): Path to the PDF document\n",
    "            chunk_size (int): Size of chunks for splitting\n",
    "            chunk_overlap (int): Overlap between chunks\n",
    "            \n",
    "        Returns:\n",
    "            AsyncEnsembleMergerRetriever: self instance for method chaining\n",
    "        \"\"\"\n",
    "        print(\"\\n[INFO] ----> Loading and splitting the document, please wait....\\n\")\n",
    "        \n",
    "        # Create loader first\n",
    "        loader = PyMuPDFLoader(data_path)\n",
    "        \n",
    "        # Load documents (run in a separate thread to not block the event loop)\n",
    "        try:\n",
    "            self.documents = await asyncio.to_thread(loader.load)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] ----> Error loading document: {e}\")\n",
    "            # Fallback to synchronous loading if needed\n",
    "            self.documents = loader.load()\n",
    "            \n",
    "        print(\"\\n[INFO] ----> Total Pages in the original document are: {}\".format(len(self.documents)))\n",
    "        \n",
    "        # Split documents (run in a separate thread)\n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=len,\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            self.document_chunks = await asyncio.to_thread(splitter.split_documents, self.documents)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] ----> Error splitting documents: {e}\")\n",
    "            # Fallback to synchronous splitting if needed\n",
    "            self.document_chunks = splitter.split_documents(self.documents)\n",
    "            \n",
    "        print(\"[INFO] ----> Total Document Chunks Created are: {}\\n\".format(len(self.document_chunks)))\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    async def setup_embeddings(self, embedding_provider=\"Google\"):\n",
    "        \"\"\"\n",
    "        Set up embeddings for vectorstores asynchronously.\n",
    "        \n",
    "        Args:\n",
    "            embedding_provider (str): Provider for embeddings (\"Google\" or \"OpenAI\")\n",
    "            \n",
    "        Returns:\n",
    "            AsyncEnsembleMergerRetriever: self instance for method chaining\n",
    "        \"\"\"\n",
    "        if embedding_provider == \"Google\":\n",
    "            print(\"[INFO] ----> Using the Google-AI Embedding model...\")\n",
    "            self.embeddings = GoogleGenerativeAIEmbeddings(\n",
    "                model=\"models/embedding-001\"\n",
    "            )\n",
    "            print(\"[INFO] ----> Google-AI Embedding model loaded.\\n\")\n",
    "        \n",
    "        elif embedding_provider == \"OpenAI\":\n",
    "            print(\"[INFO] ----> Using the Open-AI Embedding model...\")\n",
    "            self.embeddings = OpenAIEmbeddings()\n",
    "            print(\"[INFO] ----> OpenAI Embedding model loaded.\\n\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    async def create_retrievers(self, \n",
    "                         faiss_search_type=\"similarity\", \n",
    "                         faiss_k_documents=3,\n",
    "                         chroma_search_type=\"similarity\", \n",
    "                         chroma_k_documents=4):\n",
    "        \"\"\"\n",
    "        Create FAISS and Chroma retrievers asynchronously.\n",
    "        \n",
    "        Args:\n",
    "            faiss_search_type (str): Search type for FAISS retriever\n",
    "            faiss_k_documents (int): Number of documents to retrieve with FAISS\n",
    "            chroma_search_type (str): Search type for Chroma retriever\n",
    "            chroma_k_documents (int): Number of documents to retrieve with Chroma\n",
    "            \n",
    "        Returns:\n",
    "            AsyncEnsembleMergerRetriever: self instance for method chaining\n",
    "        \"\"\"\n",
    "        if self.document_chunks is None or self.embeddings is None:\n",
    "            print(\"[ERROR] ----> Documents and embeddings must be set up first\")\n",
    "            return self\n",
    "        \n",
    "        # Create both retrievers concurrently for better performance\n",
    "        print(\"[INFO] ----> Creating FAISS and Chroma Vectorstores, please wait.....\")\n",
    "        \n",
    "        # FAISS task\n",
    "        async def create_faiss():\n",
    "            try:\n",
    "                faiss_db = await asyncio.to_thread(\n",
    "                    FAISS.from_documents,\n",
    "                    self.document_chunks, \n",
    "                    self.embeddings\n",
    "                )\n",
    "                return faiss_db\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] ----> Error creating FAISS vectorstore: {e}\")\n",
    "                # Fallback to synchronous creation\n",
    "                return FAISS.from_documents(self.document_chunks, self.embeddings)\n",
    "        \n",
    "        # Chroma task\n",
    "        async def create_chroma():\n",
    "            try:\n",
    "                chroma_db = await asyncio.to_thread(\n",
    "                    Chroma.from_documents,\n",
    "                    self.document_chunks,\n",
    "                    self.embeddings,\n",
    "                )\n",
    "                return chroma_db\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] ----> Error creating Chroma vectorstore: {e}\")\n",
    "                # Fallback to synchronous creation\n",
    "                return Chroma.from_documents(self.document_chunks, self.embeddings)\n",
    "        \n",
    "        # Run both tasks concurrently\n",
    "        faiss_task = create_faiss()\n",
    "        chroma_task = create_chroma()\n",
    "        \n",
    "        # Wait for both tasks to complete\n",
    "        faiss_db, chroma_db = await asyncio.gather(faiss_task, chroma_task)\n",
    "        \n",
    "        # Set up the retrievers\n",
    "        self.faiss_retriever = faiss_db.as_retriever(\n",
    "            search_type=faiss_search_type,\n",
    "            search_kwargs={\n",
    "                \"k\": faiss_k_documents\n",
    "            }\n",
    "        )\n",
    "        print(\"[INFO] ----> FAISS Retriever Created successfully.....\\n\")\n",
    "        \n",
    "        self.chroma_retriever = chroma_db.as_retriever(\n",
    "            search_type=chroma_search_type,\n",
    "            search_kwargs={\n",
    "                \"k\": chroma_k_documents,\n",
    "            }\n",
    "        )\n",
    "        print(\"[INFO] ----> Chroma Retriever Created successfully.....\\n\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    async def build_merger_retriever(self, \n",
    "                              faiss_embedding_filter_threshold=0.75,\n",
    "                              chroma_embedding_filter_threshold=0.6):\n",
    "        \"\"\"\n",
    "        Build the ensemble merger retriever with contextual compression asynchronously.\n",
    "        \n",
    "        Args:\n",
    "            faiss_embedding_filter_threshold (float): Similarity threshold for FAISS embeddings filter\n",
    "            chroma_embedding_filter_threshold (float): Similarity threshold for Chroma embeddings filter\n",
    "            \n",
    "        Returns:\n",
    "            AsyncEnsembleMergerRetriever: self instance for method chaining\n",
    "        \"\"\"\n",
    "        if self.faiss_retriever is None or self.chroma_retriever is None:\n",
    "            print(\"[ERROR] ----> FAISS and Chroma retrievers must be created first\")\n",
    "            return self\n",
    "            \n",
    "        print(\"[INFO] ----> Building the ensemble merger retriever, please wait.....\")\n",
    "        \n",
    "        # Create shared filters\n",
    "        redundant_filter = EmbeddingsRedundantFilter(embeddings=self.embeddings)\n",
    "        reordering = LongContextReorder()\n",
    "        \n",
    "        # Create filters for FAISS\n",
    "        relevant_filter_faiss = EmbeddingsFilter(\n",
    "            embeddings=self.embeddings,\n",
    "            similarity_threshold=faiss_embedding_filter_threshold,\n",
    "        )\n",
    "        \n",
    "        # Create filters for Chroma\n",
    "        relevant_filter_chroma = EmbeddingsFilter(\n",
    "            embeddings=self.embeddings,\n",
    "            similarity_threshold=chroma_embedding_filter_threshold,\n",
    "        )\n",
    "        \n",
    "        # Create compressor pipelines\n",
    "        document_pipeline_compressor_faiss = DocumentCompressorPipeline(\n",
    "            transformers=[\n",
    "                redundant_filter,\n",
    "                relevant_filter_faiss,\n",
    "                reordering,\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        document_pipeline_compressor_chroma = DocumentCompressorPipeline(\n",
    "            transformers=[\n",
    "                redundant_filter,\n",
    "                relevant_filter_chroma,\n",
    "                reordering,\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Create compression retrievers\n",
    "        compression_retriever_with_faiss = ContextualCompressionRetriever(\n",
    "            base_compressor=document_pipeline_compressor_faiss,\n",
    "            base_retriever=self.faiss_retriever,\n",
    "        )\n",
    "        \n",
    "        compression_retriever_with_chroma = ContextualCompressionRetriever(\n",
    "            base_compressor=document_pipeline_compressor_chroma,\n",
    "            base_retriever=self.chroma_retriever,\n",
    "        )\n",
    "        \n",
    "        # Create merger retriever\n",
    "        self.merger_retriever = MergerRetriever(\n",
    "            retrievers=[\n",
    "                compression_retriever_with_faiss,\n",
    "                compression_retriever_with_chroma\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        print(\"[INFO] ----> Ensemble merger retriever built successfully.....\")\n",
    "        return self\n",
    "    \n",
    "    async def generate_answer(self, question, pipeline_type=\"RetrievalQAWithSourcesChain\", chain_type=\"stuff\", verbose=False):\n",
    "        \"\"\"\n",
    "        Generate answer using the RAG pipeline asynchronously.\n",
    "        \n",
    "        Args:\n",
    "            question (str): Question to answer\n",
    "            pipeline_type (str): Type of RAG pipeline (\"RetrievalQAWithSourcesChain\" or \"RetrievalQAChain\")\n",
    "            chain_type (str): Type of chain for retrieval QA\n",
    "            verbose (bool): Whether to print the result\n",
    "            \n",
    "        Returns:\n",
    "            str: Generated answer\n",
    "        \"\"\"\n",
    "        if self.merger_retriever is None or self.llm_model is None:\n",
    "            print(\"[ERROR] ----> Merger retriever and LLM must be set up first\")\n",
    "            return None\n",
    "            \n",
    "        print(f\"[INFO] ----> Running the {pipeline_type} Pipeline.....\\n\")\n",
    "        \n",
    "        try:\n",
    "            if pipeline_type == \"RetrievalQAWithSourcesChain\":\n",
    "                rag_pipeline = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "                    llm=self.llm_model,\n",
    "                    chain_type=chain_type,\n",
    "                    retriever=self.merger_retriever,\n",
    "                    return_source_documents=True\n",
    "                )\n",
    "                print(f\"[INFO] ----> Generating the answer for:\\nQuestion: {question}\")\n",
    "                # Run in a separate thread to not block the event loop\n",
    "                try:\n",
    "                    generated_answer = await asyncio.to_thread(rag_pipeline.invoke, question)\n",
    "                except RuntimeError as e:\n",
    "                    print(f\"[WARNING] ----> Runtime error in async execution: {e}. Falling back to synchronous.\")\n",
    "                    generated_answer = rag_pipeline.invoke(question)\n",
    "                \n",
    "                result = generated_answer[\"answer\"]\n",
    "                \n",
    "            elif pipeline_type == \"RetrievalQAChain\":\n",
    "                rag_pipeline = RetrievalQA.from_chain_type(\n",
    "                    llm=self.llm_model,\n",
    "                    chain_type=chain_type,\n",
    "                    retriever=self.merger_retriever,\n",
    "                    return_source_documents=True\n",
    "                )\n",
    "                print(f\"[INFO] ----> Generating the answer for:\\nQuestion: {question}\")\n",
    "                # Run in a separate thread to not block the event loop\n",
    "                try:\n",
    "                    generated_answer = await asyncio.to_thread(rag_pipeline.invoke, question)\n",
    "                except RuntimeError as e:\n",
    "                    print(f\"[WARNING] ----> Runtime error in async execution: {e}. Falling back to synchronous.\")\n",
    "                    generated_answer = rag_pipeline.invoke(question)\n",
    "                    \n",
    "                result = generated_answer[\"result\"]\n",
    "            \n",
    "            print(\"[INFO] ----> Result Answer generated.....\\n\")\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"Result:\\n{result}\")\n",
    "                \n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] ----> Error generating answer: {e}\")\n",
    "            return f\"Failed to generate answer due to: {str(e)}\"\n",
    "    \n",
    "    async def print_documents(self, docs):\n",
    "        \"\"\"\n",
    "        Print documents for inspection.\n",
    "        \n",
    "        Args:\n",
    "            docs (list): List of documents to print\n",
    "        \"\"\"\n",
    "        print(\n",
    "            f\"\\n{'-' * 100}\\n\".join(\n",
    "                [\n",
    "                    f\"Document {index + 1}:- \\n\\n\" + d.page_content for index, d in enumerate(docs)\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    @classmethod\n",
    "    async def create(cls, \n",
    "                    data_path, \n",
    "                    embedding_provider=\"Google\",\n",
    "                    model_name=\"gemma2-9b-it\", \n",
    "                    chunk_size=512, \n",
    "                    chunk_overlap=128,\n",
    "                    faiss_search_type=\"similarity\", \n",
    "                    faiss_k_documents=3,\n",
    "                    chroma_search_type=\"similarity\", \n",
    "                    chroma_k_documents=4,\n",
    "                    faiss_embedding_filter_threshold=0.75,\n",
    "                    chroma_embedding_filter_threshold=0.6):\n",
    "        \"\"\"\n",
    "        Factory method to create and setup the retriever in one go.\n",
    "        \n",
    "        Args:\n",
    "            data_path (str): Path to the PDF document\n",
    "            embedding_provider (str): Provider for embeddings (\"Google\" or \"OpenAI\")\n",
    "            model_name (str): Model name for LLM\n",
    "            chunk_size (int): Size of chunks for splitting\n",
    "            chunk_overlap (int): Overlap between chunks\n",
    "            faiss_search_type (str): Search type for FAISS retriever\n",
    "            faiss_k_documents (int): Number of documents to retrieve with FAISS\n",
    "            chroma_search_type (str): Search type for Chroma retriever\n",
    "            chroma_k_documents (int): Number of documents to retrieve with Chroma\n",
    "            faiss_embedding_filter_threshold (float): Similarity threshold for FAISS embeddings\n",
    "            chroma_embedding_filter_threshold (float): Similarity threshold for Chroma embeddings\n",
    "            \n",
    "        Returns:\n",
    "            AsyncEnsembleMergerRetriever: Fully configured instance\n",
    "        \"\"\"\n",
    "        instance = cls()\n",
    "        \n",
    "        # Setup in parallel\n",
    "        await instance.setup_llm(model_name=model_name)\n",
    "        await instance.load_and_split_documents(\n",
    "            data_path=data_path,\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap\n",
    "        )\n",
    "        await instance.setup_embeddings(embedding_provider=embedding_provider)\n",
    "        \n",
    "        # These operations depend on the previous ones\n",
    "        await instance.create_retrievers(\n",
    "            faiss_search_type=faiss_search_type,\n",
    "            faiss_k_documents=faiss_k_documents,\n",
    "            chroma_search_type=chroma_search_type,\n",
    "            chroma_k_documents=chroma_k_documents\n",
    "        )\n",
    "        \n",
    "        await instance.build_merger_retriever(\n",
    "            faiss_embedding_filter_threshold=faiss_embedding_filter_threshold,\n",
    "            chroma_embedding_filter_threshold=chroma_embedding_filter_threshold\n",
    "        )\n",
    "        \n",
    "        return instance\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "async def main():\n",
    "    # Method 1: Step by step\n",
    "    retriever = AsyncEnsembleMergerRetriever()\n",
    "    \n",
    "    # Set up components asynchronously\n",
    "    await retriever.setup_llm(model_name=\"gemma2-9b-it\")\n",
    "    await retriever.load_and_split_documents(\n",
    "        data_path=\"Data\\\\ReAct.pdf\",\n",
    "        chunk_size=512,\n",
    "        chunk_overlap=128\n",
    "    )\n",
    "\n",
    "    await retriever.setup_embeddings(embedding_provider=\"Google\")\n",
    "\n",
    "    await retriever.create_retrievers(\n",
    "        faiss_search_type=\"similarity\",\n",
    "        faiss_k_documents=3,\n",
    "        chroma_search_type=\"mmr\",\n",
    "        chroma_k_documents=5\n",
    "    )\n",
    "\n",
    "    await retriever.build_merger_retriever(\n",
    "        faiss_embedding_filter_threshold=0.75,\n",
    "        chroma_embedding_filter_threshold=0.75,\n",
    "    )\n",
    "    \n",
    "    # Generate answer\n",
    "    query = \"Explain in details about the Chain of thought prompting as mentioned in ReAct Paper.\"\n",
    "    answer = await retriever.generate_answer(\n",
    "        question=query,\n",
    "        pipeline_type=\"RetrievalQAWithSourcesChain\",\n",
    "        chain_type=\"stuff\",\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(\"===\" * 100)\n",
    "    print(f\"Final Answer:\\n{answer}\")\n",
    "    \n",
    "    # Method 2: Using the factory method\n",
    "    print(\"\\n\\nCreating a new retriever using the factory method...\")\n",
    "    retriever2 = await AsyncEnsembleMergerRetriever.create(\n",
    "        data_path=\"Data\\\\ReAct.pdf\",\n",
    "        embedding_provider=\"Google\",\n",
    "        model_name=\"gemma2-9b-it\"\n",
    "    )\n",
    "    \n",
    "    answer2 = await retriever2.generate_answer(\n",
    "        question=query,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "def run_async_retriever():\n",
    "    \"\"\"\n",
    "    Helper function to safely run the async code.\n",
    "    This handles cases where it might be called from inside another event loop.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Try to run using asyncio.run() (works when no event loop is running)\n",
    "        asyncio.run(main())\n",
    "    except RuntimeError:\n",
    "        # If we're already in an event loop, get the current loop and run the coroutine\n",
    "        loop = asyncio.get_event_loop()\n",
    "        loop.create_task(main())\n",
    "        # Note: In a Jupyter notebook or interactive environment, \n",
    "        # you may need additional handling\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     run_async_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c25cf7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] ----> Loading and splitting the document, please wait....\n",
      "\n",
      "\n",
      "[INFO] ----> Total Pages in the original document are: 33\n",
      "[INFO] ----> Total Document Chunks Created are: 293\n",
      "\n",
      "[INFO] ----> Using the Google-AI Embedding model...\n",
      "[INFO] ----> Google-AI Embedding model loaded.\n",
      "\n",
      "[INFO] ----> Creating FAISS and Chroma Vectorstores, please wait.....\n",
      "[INFO] ----> FAISS Retriever Created successfully.....\n",
      "\n",
      "[INFO] ----> Chroma Retriever Created successfully.....\n",
      "\n",
      "[INFO] ----> Building the ensemble merger retriever, please wait.....\n",
      "[INFO] ----> Ensemble merger retriever built successfully.....\n"
     ]
    }
   ],
   "source": [
    "# Create and use the retriever asynchronously\n",
    "retriever = await AsyncEnsembleMergerRetriever.create(\n",
    "    data_path=\"Data\\\\ReAct.pdf\",\n",
    "    embedding_provider=\"Google\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c016e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] ----> Running the RetrievalQAWithSourcesChain Pipeline.....\n",
      "\n",
      "[INFO] ----> Generating the answer for:\n",
      "Question: Explain with detail about the ReAct\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RetrievalQAWithSourcesChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"Explain with detail about the ReAct\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RetrievalQAWithSourcesChain > chain:StuffDocumentsChain] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RetrievalQAWithSourcesChain > chain:StuffDocumentsChain > chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"Explain with detail about the ReAct\",\n",
      "  \"summaries\": \"Content: when ﬁnetuning is enabled, and in Section 4 how ReAct performance is robust to prompt selections.\\nD) Human aligned and controllable: ReAct promises an interpretable sequential decision making\\nand reasoning process where humans can easily inspect reasoning and factual correctness. Moreover,\\nhumans can also control or correct the agent behavior on the go by thought editing, as shown in\\nFigure 5 in Section 4.\\n3\\nKNOWLEDGE-INTENSIVE REASONING TASKS\\nSource: Data\\\\ReAct.pdf\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RetrievalQAWithSourcesChain > chain:StuffDocumentsChain > chain:LLMChain > llm:ChatGroq] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Given the following extracted parts of a long document and a question, create a final answer with references (\\\"SOURCES\\\"). \\nIf you don't know the answer, just say that you don't know. Don't try to make up an answer.\\nALWAYS return a \\\"SOURCES\\\" part in your answer.\\n\\nQUESTION: Which state/country's law governs the interpretation of the contract?\\n=========\\nContent: This Agreement is governed by English law and the parties submit to the exclusive jurisdiction of the English courts in  relation to any dispute (contractual or non-contractual) concerning this Agreement save that either party may apply to any court for an  injunction or other relief to protect its Intellectual Property Rights.\\nSource: 28-pl\\nContent: No Waiver. Failure or delay in exercising any right or remedy under this Agreement shall not constitute a waiver of such (or any other)  right or remedy.\\n\\n11.7 Severability. The invalidity, illegality or unenforceability of any term (or part of a term) of this Agreement shall not affect the continuation  in force of the remainder of the term (if any) and this Agreement.\\n\\n11.8 No Agency. Except as expressly stated otherwise, nothing in this Agreement shall create an agency, partnership or joint venture of any  kind between the parties.\\n\\n11.9 No Third-Party Beneficiaries.\\nSource: 30-pl\\nContent: (b) if Google believes, in good faith, that the Distributor has violated or caused Google to violate any Anti-Bribery Laws (as  defined in Clause 8.5) or that such a violation is reasonably likely to occur,\\nSource: 4-pl\\n=========\\nFINAL ANSWER: This Agreement is governed by English law.\\nSOURCES: 28-pl\\n\\nQUESTION: What did the president say about Michael Jackson?\\n=========\\nContent: Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.  \\n\\nLast year COVID-19 kept us apart. This year we are finally together again. \\n\\nTonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. \\n\\nWith a duty to one another to the American people to the Constitution. \\n\\nAnd with an unwavering resolve that freedom will always triumph over tyranny. \\n\\nSix days ago, Russia’s Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways. But he badly miscalculated. \\n\\nHe thought he could roll into Ukraine and the world would roll over. Instead he met a wall of strength he never imagined. \\n\\nHe met the Ukrainian people. \\n\\nFrom President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world. \\n\\nGroups of citizens blocking tanks with their bodies. Everyone from students to retirees teachers turned soldiers defending their homeland.\\nSource: 0-pl\\nContent: And we won’t stop. \\n\\nWe have lost so much to COVID-19. Time with one another. And worst of all, so much loss of life. \\n\\nLet’s use this moment to reset. Let’s stop looking at COVID-19 as a partisan dividing line and see it for what it is: A God-awful disease.  \\n\\nLet’s stop seeing each other as enemies, and start seeing each other for who we really are: Fellow Americans.  \\n\\nWe can’t change how divided we’ve been. But we can change how we move forward—on COVID-19 and other issues we must face together. \\n\\nI recently visited the New York City Police Department days after the funerals of Officer Wilbert Mora and his partner, Officer Jason Rivera. \\n\\nThey were responding to a 9-1-1 call when a man shot and killed them with a stolen gun. \\n\\nOfficer Mora was 27 years old. \\n\\nOfficer Rivera was 22. \\n\\nBoth Dominican Americans who’d grown up on the same streets they later chose to patrol as police officers. \\n\\nI spoke with their families and told them that we are forever in debt for their sacrifice, and we will carry on their mission to restore the trust and safety every community deserves.\\nSource: 24-pl\\nContent: And a proud Ukrainian people, who have known 30 years  of independence, have repeatedly shown that they will not tolerate anyone who tries to take their country backwards.  \\n\\nTo all Americans, I will be honest with you, as I’ve always promised. A Russian dictator, invading a foreign country, has costs around the world. \\n\\nAnd I’m taking robust action to make sure the pain of our sanctions  is targeted at Russia’s economy. And I will use every tool at our disposal to protect American businesses and consumers. \\n\\nTonight, I can announce that the United States has worked with 30 other countries to release 60 Million barrels of oil from reserves around the world.  \\n\\nAmerica will lead that effort, releasing 30 Million barrels from our own Strategic Petroleum Reserve. And we stand ready to do more if necessary, unified with our allies.  \\n\\nThese steps will help blunt gas prices here at home. And I know the news about what’s happening can seem alarming. \\n\\nBut I want you to know that we are going to be okay.\\nSource: 5-pl\\nContent: More support for patients and families. \\n\\nTo get there, I call on Congress to fund ARPA-H, the Advanced Research Projects Agency for Health. \\n\\nIt’s based on DARPA—the Defense Department project that led to the Internet, GPS, and so much more.  \\n\\nARPA-H will have a singular purpose—to drive breakthroughs in cancer, Alzheimer’s, diabetes, and more. \\n\\nA unity agenda for the nation. \\n\\nWe can do this. \\n\\nMy fellow Americans—tonight , we have gathered in a sacred space—the citadel of our democracy. \\n\\nIn this Capitol, generation after generation, Americans have debated great questions amid great strife, and have done great things. \\n\\nWe have fought for freedom, expanded liberty, defeated totalitarianism and terror. \\n\\nAnd built the strongest, freest, and most prosperous nation the world has ever known. \\n\\nNow is the hour. \\n\\nOur moment of responsibility. \\n\\nOur test of resolve and conscience, of history itself. \\n\\nIt is in this moment that our character is formed. Our purpose is found. Our future is forged. \\n\\nWell I know this nation.\\nSource: 34-pl\\n=========\\nFINAL ANSWER: The president did not mention Michael Jackson.\\nSOURCES:\\n\\nQUESTION: Explain with detail about the ReAct\\n=========\\nContent: when ﬁnetuning is enabled, and in Section 4 how ReAct performance is robust to prompt selections.\\nD) Human aligned and controllable: ReAct promises an interpretable sequential decision making\\nand reasoning process where humans can easily inspect reasoning and factual correctness. Moreover,\\nhumans can also control or correct the agent behavior on the go by thought editing, as shown in\\nFigure 5 in Section 4.\\n3\\nKNOWLEDGE-INTENSIVE REASONING TASKS\\nSource: Data\\\\ReAct.pdf\\n=========\\nFINAL ANSWER:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RetrievalQAWithSourcesChain > chain:StuffDocumentsChain > chain:LLMChain > llm:ChatGroq] [648ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"ReAct is a human-aligned and controllable sequential decision-making and reasoning system. \\n\\nIt offers an interpretable process that allows humans to inspect the reasoning and factual correctness behind the agent's decisions. \\n\\nFurthermore, humans can control or correct the agent's behavior in real-time through a technique called \\\"thought editing,\\\" as demonstrated in Figure 5.\\n\\nSOURCES: Data\\\\ReAct.pdf \\n\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"ReAct is a human-aligned and controllable sequential decision-making and reasoning system. \\n\\nIt offers an interpretable process that allows humans to inspect the reasoning and factual correctness behind the agent's decisions. \\n\\nFurthermore, humans can control or correct the agent's behavior in real-time through a technique called \\\"thought editing,\\\" as demonstrated in Figure 5.\\n\\nSOURCES: Data\\\\ReAct.pdf \\n\",\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 88,\n",
      "                \"prompt_tokens\": 1548,\n",
      "                \"total_tokens\": 1636,\n",
      "                \"completion_time\": 0.16,\n",
      "                \"prompt_time\": 0.055956816,\n",
      "                \"queue_time\": 0.247172574,\n",
      "                \"total_time\": 0.215956816\n",
      "              },\n",
      "              \"model_name\": \"gemma2-9b-it\",\n",
      "              \"system_fingerprint\": \"fp_10c08bf97d\",\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-7e13e8cb-51fd-4b9c-a95b-4c93c76c4558-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 1548,\n",
      "              \"output_tokens\": 88,\n",
      "              \"total_tokens\": 1636\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 88,\n",
      "      \"prompt_tokens\": 1548,\n",
      "      \"total_tokens\": 1636,\n",
      "      \"completion_time\": 0.16,\n",
      "      \"prompt_time\": 0.055956816,\n",
      "      \"queue_time\": 0.247172574,\n",
      "      \"total_time\": 0.215956816\n",
      "    },\n",
      "    \"model_name\": \"gemma2-9b-it\",\n",
      "    \"system_fingerprint\": \"fp_10c08bf97d\"\n",
      "  },\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RetrievalQAWithSourcesChain > chain:StuffDocumentsChain > chain:LLMChain] [648ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"ReAct is a human-aligned and controllable sequential decision-making and reasoning system. \\n\\nIt offers an interpretable process that allows humans to inspect the reasoning and factual correctness behind the agent's decisions. \\n\\nFurthermore, humans can control or correct the agent's behavior in real-time through a technique called \\\"thought editing,\\\" as demonstrated in Figure 5.\\n\\nSOURCES: Data\\\\ReAct.pdf \\n\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RetrievalQAWithSourcesChain > chain:StuffDocumentsChain] [664ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output_text\": \"ReAct is a human-aligned and controllable sequential decision-making and reasoning system. \\n\\nIt offers an interpretable process that allows humans to inspect the reasoning and factual correctness behind the agent's decisions. \\n\\nFurthermore, humans can control or correct the agent's behavior in real-time through a technique called \\\"thought editing,\\\" as demonstrated in Figure 5.\\n\\nSOURCES: Data\\\\ReAct.pdf \\n\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RetrievalQAWithSourcesChain] [3.14s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "[INFO] ----> Result Answer generated.....\n",
      "\n",
      "\n",
      "\n",
      " ReAct is a human-aligned and controllable sequential decision-making and reasoning system. \n",
      "\n",
      "It offers an interpretable process that allows humans to inspect the reasoning and factual correctness behind the agent's decisions. \n",
      "\n",
      "Furthermore, humans can control or correct the agent's behavior in real-time through a technique called \"thought editing,\" as demonstrated in Figure 5.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "langchain.debug=True\n",
    "\n",
    "answer = await retriever.generate_answer(\"Explain with detail about the ReAct\")\n",
    "print(\"\\n\\n\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90aa86b1",
   "metadata": {},
   "source": [
    " ReAct is a human-aligned and controllable sequential decision-making and reasoning system. \n",
    "\n",
    "It offers an interpretable process that allows humans to inspect the reasoning and factual correctness behind the agent's decisions. \n",
    "\n",
    "Furthermore, humans can control or correct the agent's behavior in real-time through a technique called \"thought editing,\" as demonstrated in Figure 5.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2029ee3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hybrid-rag-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
